{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#MLP to Classify Iris (1 hidden layer, 100 hidden neurons)"
      ],
      "metadata": {
        "id": "4VCPLuVmxcbE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-00-X1ZdxaBd",
        "outputId": "a680d2f0-de12-4501-c1c3-918d23656649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initializing MLP Classifier\n",
        "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
        "\n",
        "# Training the classifier\n",
        "mlp_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on the testing set\n",
        "predictions = mlp_clf.predict(X_test)\n",
        "\n",
        "# Accuracy calculation\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Something to note for above code:\n",
        "\n",
        "* **Iris Dataset:** This is a classic dataset in the field of machine learning, often used for testing classification algorithms. It consists of 150 samples from three species of Iris (Setosa, Versicolor, and Virginica), with four features: sepal length, sepal width, petal length, and petal width.\n",
        "\n",
        "* **Data Splitting:** The dataset is divided into a training set (80% of the data) and a test set (20% of the data). This is done to validate the performance of the model on unseen data, thereby giving an indication of how well the model generalizes.\n",
        "\n",
        "* **Multi-Layer Perceptron (MLP) Classifier:** The MLP is a type of neural network that is well-suited for complex pattern recognition tasks. The model in the code uses a single hidden layer with 100 neurons, which is a relatively simple architecture.\n",
        "\n",
        "* **Activation Function:** The relu (Rectified Linear Unit) activation function is used. It's a popular choice due to its simplicity and effectiveness in many scenarios, as it helps with the vanishing gradient problem and allows the model to learn non-linear patterns.\n",
        "\n",
        "* **Solver:** The adam solver is an adaptive learning rate optimization algorithm. It's widely used because it combines the advantages of two other extensions of stochastic gradient descent, namely AdaGrad and RMSProp, and works well in practice with relatively little tuning.\n",
        "\n",
        "* **Maximum Iterations:** The max_iter parameter is set to 1000. This is the number of epochs to run the optimization algorithm. If the algorithm converges before reaching the maximum number of iterations, the training will stop early.\n",
        "\n",
        "* **Random State:** By setting the random_state parameter to 42 for both the train-test split and the MLPClassifier, the shuffling and weight initialization are reproducible. This is good for debugging and comparing models, as it ensures that the random number generation is consistent across runs.\n",
        "\n",
        "* **Performance:** The accuracy score is 1.0, which means the model has perfectly classified all the instances in the test set. While this might indicate excellent performance, it could also be a sign of overfitting, especially if the test set is too small or not representative of the general population.\n",
        "\n",
        "# General Considerations:\n",
        "\n",
        "While high accuracy on the test set is desirable, it's essential to check for overfitting by using additional validation methods such as cross-validation.\n",
        "Given that the Iris dataset is small and not very complex, a neural network with a large number of hidden neurons might be excessive. Simpler models, such as logistic regression or a decision tree, might perform just as well.\n",
        "Regularization techniques, such as L2 regularization (parameter alpha in MLPClassifier), dropout, or early stopping, could be used to prevent overfitting if it were a concern.\n",
        "\n",
        "In a real-world scenario, it's crucial to perform further model evaluation, potentially using a validation set or employing cross-validation techniques, along with a thorough analysis of the classification report, confusion matrix, and other metrics beyond accuracy."
      ],
      "metadata": {
        "id": "gAa95jKBx0Wn"
      }
    }
  ]
}